{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reinforce_tf.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GyTlHSbYOrsg"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "####from networks import PolicyGradientNetwork"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "  def __init__(self, alpha = 0.003, gamma = 0.99, n_actions = 4,\n",
        "               fc1_dims = 256, fc2_dims = 256):\n",
        "    # SAVE THE PARAMETERS\n",
        "    self.alpha = alpha\n",
        "    self.gamma = gamma\n",
        "    self.n_actions = n_actions\n",
        "    self.state_memory = []\n",
        "    self.reward_memory = []\n",
        "    self.policy = PolicyGradientNetwork(n_actions = n_actions)\n",
        "    self.policy.compile(optimizer = Adam(learning_rate = self.alpha))\n",
        "\n",
        "\n",
        "  def choose_action(self, observation):\n",
        "    state = tf.convert_to_tensor([observation], dtype = tf.float32)\n",
        "    probs = self.policy(state)\n",
        "    action_probs = tfp.distributions.Categorical(probs = probs)\n",
        "    action = action_probs.sample()\n",
        "\n",
        "    return action.numpy()[0]\n",
        "\n",
        "\n",
        "  def store_transition(self, observation, action, reward):\n",
        "    self.state_memory.append(observation)\n",
        "    self.action_memory.append(action)\n",
        "    self.reward_memory.append(reward)\n",
        "\n",
        "\n",
        "  def learn(self):\n",
        "    actions = tf.convert_to_tensor(self.acton_memory, dtype = tf.float32)\n",
        "    rewards = tf.covert_to_tensor(self.reward_memory)\n",
        "\n",
        "    G = np.zeros_like(rewards)\n",
        "    for t in range(len(rewards)):\n",
        "      G_sum = 0\n",
        "      discount = 1\n",
        "      for k in range(t, len(rewards)):\n",
        "        G_sum += rewards[k] * discount\n",
        "        discount *= self.gamma\n",
        "\n",
        "      G[t] = G_sum\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      loss = 0\n",
        "      for idx, (g, state) in enumerate(zip(G, self.state_memory)):\n",
        "        state = tf.convert_to_tensor([state], dtype = tf.float32)\n",
        "        probs = self.policy(state)\n",
        "        action_probs = tfp.distributions.Categorical(probs = probs)\n",
        "        log_prob = action_probs.log_prob(action[idx])\n",
        "        loss += -g * tf.squeeze(log_prob)\n",
        "\n",
        "    gradient = tape.gradient(loss, self.policy.trainable_variables)\n",
        "    self.policy.optimizer.apply_gradients(zip(gradient,\n",
        "                                              self.policy.trainable_variables))\n",
        "    \n",
        "    self.state_memory = []\n",
        "    self.action_memory = []\n",
        "    self.reward_memory = []\n",
        "\n"
      ],
      "metadata": {
        "id": "Fhci1A4iPSPJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}